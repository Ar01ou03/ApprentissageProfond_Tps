{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63288861-e949-4921-9757-127ae05cde5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True, render_mode=\"human\")\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0656d5f2-af6f-4a3b-bd17-9dd286901b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'états : 16\n",
      "Nombre d'actions : 4\n"
     ]
    }
   ],
   "source": [
    "print(\"Nombre d'états :\", env.observation_space.n)\n",
    "print(\"Nombre d'actions :\", env.action_space.n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f5bcecf-c63d-4ca6-8c09-a1483c8edadd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Épisode 1\n",
      "--------------------\n",
      "Action: 0, État suivant: 4, Récompense: 0.0\n",
      "Action: 2, État suivant: 5, Récompense: 0.0\n",
      "Épisode terminé.\n",
      "\n",
      "\n",
      "Épisode 2\n",
      "--------------------\n",
      "Action: 3, État suivant: 0, Récompense: 0.0\n",
      "Action: 2, État suivant: 4, Récompense: 0.0\n",
      "Action: 3, État suivant: 4, Récompense: 0.0\n",
      "Action: 2, État suivant: 0, Récompense: 0.0\n",
      "Action: 1, État suivant: 4, Récompense: 0.0\n",
      "Action: 1, État suivant: 4, Récompense: 0.0\n",
      "Action: 3, État suivant: 5, Récompense: 0.0\n",
      "Épisode terminé.\n",
      "\n",
      "\n",
      "Épisode 3\n",
      "--------------------\n",
      "Action: 2, État suivant: 1, Récompense: 0.0\n",
      "Action: 1, État suivant: 0, Récompense: 0.0\n",
      "Action: 2, État suivant: 1, Récompense: 0.0\n",
      "Action: 2, État suivant: 5, Récompense: 0.0\n",
      "Épisode terminé.\n",
      "\n",
      "\n",
      "Épisode 4\n",
      "--------------------\n",
      "Action: 1, État suivant: 0, Récompense: 0.0\n",
      "Action: 1, État suivant: 0, Récompense: 0.0\n",
      "Action: 3, État suivant: 1, Récompense: 0.0\n",
      "Action: 3, État suivant: 0, Récompense: 0.0\n",
      "Action: 3, État suivant: 1, Récompense: 0.0\n",
      "Action: 1, État suivant: 2, Récompense: 0.0\n",
      "Action: 0, État suivant: 1, Récompense: 0.0\n",
      "Action: 1, État suivant: 2, Récompense: 0.0\n",
      "Action: 0, État suivant: 6, Récompense: 0.0\n",
      "Action: 2, État suivant: 10, Récompense: 0.0\n",
      "Action: 0, État suivant: 14, Récompense: 0.0\n",
      "Action: 3, État suivant: 13, Récompense: 0.0\n",
      "Action: 1, État suivant: 14, Récompense: 0.0\n",
      "Action: 3, État suivant: 10, Récompense: 0.0\n",
      "Action: 1, État suivant: 9, Récompense: 0.0\n",
      "Action: 1, État suivant: 10, Récompense: 0.0\n",
      "Action: 1, État suivant: 9, Récompense: 0.0\n",
      "Action: 3, État suivant: 10, Récompense: 0.0\n",
      "Action: 3, État suivant: 9, Récompense: 0.0\n",
      "Action: 1, État suivant: 8, Récompense: 0.0\n",
      "Action: 2, État suivant: 12, Récompense: 0.0\n",
      "Épisode terminé.\n",
      "\n",
      "\n",
      "Épisode 5\n",
      "--------------------\n",
      "Action: 0, État suivant: 0, Récompense: 0.0\n",
      "Action: 2, État suivant: 4, Récompense: 0.0\n",
      "Action: 1, État suivant: 4, Récompense: 0.0\n",
      "Action: 3, État suivant: 4, Récompense: 0.0\n",
      "Action: 1, État suivant: 4, Récompense: 0.0\n",
      "Action: 3, État suivant: 5, Récompense: 0.0\n",
      "Épisode terminé.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 5\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    print(f\"\\nÉpisode {episode + 1}\\n{'-' * 20}\")\n",
    "\n",
    "    while not done:\n",
    "        # Choisir une action aléatoire\n",
    "        action = env.action_space.sample()\n",
    "\n",
    "        # Appliquer l’action\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "        print(f\"Action: {action}, État suivant: {next_state}, Récompense: {reward}\")\n",
    "\n",
    "        # Si l'épisode est terminé, sortir de la boucle\n",
    "        if done or truncated:\n",
    "            print(\"Épisode terminé.\\n\")\n",
    "            break\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a3ed699-c872-4a62-b525-0e89cff431b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Épisode 1\n",
      "--------------------\n",
      "Action: 0, État suivant: 0, Récompense: 0.0\n",
      "Action: 3, État suivant: 0, Récompense: 0.0\n",
      "Action: 2, État suivant: 1, Récompense: 0.0\n",
      "Action: 2, État suivant: 2, Récompense: 0.0\n",
      "Action: 2, État suivant: 3, Récompense: 0.0\n",
      "Action: 0, État suivant: 2, Récompense: 0.0\n",
      "Action: 0, État suivant: 6, Récompense: 0.0\n",
      "Action: 0, État suivant: 10, Récompense: 0.0\n",
      "Action: 0, État suivant: 9, Récompense: 0.0\n",
      "Action: 3, État suivant: 5, Récompense: 0.0\n",
      "Épisode terminé.\n",
      "\n",
      "\n",
      "Épisode 2\n",
      "--------------------\n",
      "Action: 0, État suivant: 0, Récompense: 0.0\n",
      "Action: 2, État suivant: 0, Récompense: 0.0\n",
      "Action: 1, État suivant: 4, Récompense: 0.0\n",
      "Action: 1, État suivant: 8, Récompense: 0.0\n",
      "Action: 3, État suivant: 9, Récompense: 0.0\n",
      "Action: 0, État suivant: 5, Récompense: 0.0\n",
      "Épisode terminé.\n",
      "\n",
      "\n",
      "Épisode 3\n",
      "--------------------\n",
      "Action: 0, État suivant: 0, Récompense: 0.0\n",
      "Action: 3, État suivant: 0, Récompense: 0.0\n",
      "Action: 2, État suivant: 0, Récompense: 0.0\n",
      "Action: 2, État suivant: 0, Récompense: 0.0\n",
      "Action: 1, État suivant: 1, Récompense: 0.0\n",
      "Action: 3, État suivant: 0, Récompense: 0.0\n",
      "Action: 1, État suivant: 4, Récompense: 0.0\n",
      "Action: 0, État suivant: 4, Récompense: 0.0\n",
      "Action: 1, État suivant: 5, Récompense: 0.0\n",
      "Épisode terminé.\n",
      "\n",
      "\n",
      "Épisode 4\n",
      "--------------------\n",
      "Action: 3, État suivant: 0, Récompense: 0.0\n",
      "Action: 2, État suivant: 1, Récompense: 0.0\n",
      "Action: 1, État suivant: 2, Récompense: 0.0\n",
      "Action: 0, État suivant: 6, Récompense: 0.0\n",
      "Action: 1, État suivant: 7, Récompense: 0.0\n",
      "Épisode terminé.\n",
      "\n",
      "\n",
      "Épisode 5\n",
      "--------------------\n",
      "Action: 1, État suivant: 1, Récompense: 0.0\n",
      "Action: 3, État suivant: 2, Récompense: 0.0\n",
      "Action: 3, État suivant: 2, Récompense: 0.0\n",
      "Action: 1, État suivant: 1, Récompense: 0.0\n",
      "Action: 2, État suivant: 2, Récompense: 0.0\n",
      "Action: 3, État suivant: 2, Récompense: 0.0\n",
      "Action: 2, État suivant: 2, Récompense: 0.0\n",
      "Action: 3, État suivant: 2, Récompense: 0.0\n",
      "Action: 0, État suivant: 1, Récompense: 0.0\n",
      "Action: 0, État suivant: 5, Récompense: 0.0\n",
      "Épisode terminé.\n",
      "\n",
      "\n",
      "Épisode 6\n",
      "--------------------\n",
      "Action: 0, État suivant: 0, Récompense: 0.0\n",
      "Action: 0, État suivant: 0, Récompense: 0.0\n",
      "Action: 0, État suivant: 0, Récompense: 0.0\n",
      "Action: 0, État suivant: 0, Récompense: 0.0\n",
      "Action: 2, État suivant: 0, Récompense: 0.0\n",
      "Action: 2, État suivant: 4, Récompense: 0.0\n",
      "Action: 1, État suivant: 4, Récompense: 0.0\n",
      "Action: 1, État suivant: 4, Récompense: 0.0\n",
      "Action: 0, État suivant: 0, Récompense: 0.0\n",
      "Action: 1, État suivant: 4, Récompense: 0.0\n",
      "Action: 0, État suivant: 8, Récompense: 0.0\n",
      "Action: 2, État suivant: 12, Récompense: 0.0\n",
      "Épisode terminé.\n",
      "\n",
      "\n",
      "Épisode 7\n",
      "--------------------\n",
      "Action: 1, État suivant: 0, Récompense: 0.0\n",
      "Action: 0, État suivant: 0, Récompense: 0.0\n",
      "Action: 1, État suivant: 4, Récompense: 0.0\n",
      "Action: 3, État suivant: 0, Récompense: 0.0\n",
      "Action: 2, État suivant: 1, Récompense: 0.0\n",
      "Action: 1, État suivant: 2, Récompense: 0.0\n",
      "Action: 1, État suivant: 1, Récompense: 0.0\n",
      "Action: 3, État suivant: 0, Récompense: 0.0\n",
      "Action: 1, État suivant: 4, Récompense: 0.0\n",
      "Action: 2, État suivant: 5, Récompense: 0.0\n",
      "Épisode terminé.\n",
      "\n",
      "\n",
      "Épisode 8\n",
      "--------------------\n",
      "Action: 3, État suivant: 0, Récompense: 0.0\n",
      "Action: 0, État suivant: 0, Récompense: 0.0\n",
      "Action: 3, État suivant: 0, Récompense: 0.0\n",
      "Action: 2, État suivant: 0, Récompense: 0.0\n",
      "Action: 3, État suivant: 1, Récompense: 0.0\n",
      "Action: 2, État suivant: 2, Récompense: 0.0\n",
      "Action: 3, État suivant: 1, Récompense: 0.0\n",
      "Action: 3, État suivant: 2, Récompense: 0.0\n",
      "Action: 1, État suivant: 1, Récompense: 0.0\n",
      "Action: 2, État suivant: 5, Récompense: 0.0\n",
      "Épisode terminé.\n",
      "\n",
      "\n",
      "Épisode 9\n",
      "--------------------\n",
      "Action: 1, État suivant: 4, Récompense: 0.0\n",
      "Action: 2, État suivant: 5, Récompense: 0.0\n",
      "Épisode terminé.\n",
      "\n",
      "\n",
      "Épisode 10\n",
      "--------------------\n",
      "Action: 0, État suivant: 0, Récompense: 0.0\n",
      "Action: 3, État suivant: 0, Récompense: 0.0\n",
      "Action: 1, État suivant: 0, Récompense: 0.0\n",
      "Action: 0, État suivant: 0, Récompense: 0.0\n",
      "Action: 3, État suivant: 1, Récompense: 0.0\n",
      "Action: 0, État suivant: 1, Récompense: 0.0\n",
      "Action: 1, État suivant: 0, Récompense: 0.0\n",
      "Action: 3, État suivant: 1, Récompense: 0.0\n",
      "Action: 1, État suivant: 5, Récompense: 0.0\n",
      "Épisode terminé.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# Charger l’environnement FrozenLake\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "\n",
    "num_episodes = 10\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    print(f\"\\nÉpisode {episode + 1}\\n{'-' * 20}\")\n",
    "\n",
    "    while not done:\n",
    "        # Choisir une action aléatoire\n",
    "        action = env.action_space.sample()\n",
    "\n",
    "        # Appliquer l’action\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "        print(f\"Action: {action}, État suivant: {next_state}, Récompense: {reward}\")\n",
    "\n",
    "        # Si l'épisode est terminé, sortir de la boucle\n",
    "        if done or truncated:\n",
    "            print(\"Épisode terminé.\\n\")\n",
    "            break\n",
    " \n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8bfad93-f748-4619-86a0-4e7a8e783325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-Table initialisée :\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "# Charger l'environnement FrozenLake\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "\n",
    "# Obtenir le nombre d'états et d'actions\n",
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "# Créer et initialiser la Q-Table à 0\n",
    "q_table = np.zeros((num_states, num_actions))\n",
    "\n",
    "print(\"Q-Table initialisée :\")\n",
    "print(q_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96b9a298-1882-4bef-ae92-7c723f199777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-Table après apprentissage :\n",
      "[[0.10562349 0.10162517 0.10386433 0.10693768]\n",
      " [0.09812903 0.08644038 0.0697654  0.10524712]\n",
      " [0.10030592 0.09897262 0.09394128 0.10056544]\n",
      " [0.03435376 0.03357259 0.03678962 0.09940053]\n",
      " [0.10981433 0.04033252 0.05643914 0.04161711]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.13977526 0.04764047 0.06463965 0.05575012]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.02965009 0.06231894 0.11929298 0.05569826]\n",
      " [0.04663432 0.25762811 0.04603051 0.0666746 ]\n",
      " [0.29950322 0.20570754 0.09592647 0.11114244]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.02519346 0.         0.         0.21178676]\n",
      " [0.17729454 0.4105384  0.75826294 0.22648988]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Paramètres\n",
    "alpha = 0.1  # Taux d'apprentissage\n",
    "gamma = 0.99  # Facteur de discount\n",
    "epsilon = 1.0  # Exploration initiale\n",
    "epsilon_decay = 0.995  # Décroissance d'epsilon\n",
    "num_episodes = 5000  # Nombre d'épisodes\n",
    "\n",
    "# Boucle d'apprentissage\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()[0]  # État initial\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])\n",
    "\n",
    "\n",
    "        new_state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "        # Mettre à jour la Q-Table\n",
    "        q_table[state, action] = q_table[state, action] + alpha * (\n",
    "            reward + gamma * np.max(q_table[new_state]) - q_table[state, action]\n",
    "        )\n",
    "\n",
    "        # Mettre à jour l'état\n",
    "        state = new_state\n",
    "\n",
    "    # Réduire epsilon (moins d'exploration au fil du temps)\n",
    "    epsilon = max(0.01, epsilon * epsilon_decay)\n",
    "\n",
    "# Affichage de la Q-Table après l'apprentissage\n",
    "print(\"Q-Table après apprentissage :\")\n",
    "print(q_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9f42f1-0954-424f-b38a-6bcc2dd75a57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
