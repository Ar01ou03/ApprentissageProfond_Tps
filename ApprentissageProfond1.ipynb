{
 "cells": [
  {
   "cell_type": "raw",
   "id": "be9b3d00-c65f-4645-8653-5fda345c5c6d",
   "metadata": {},
   "source": [
    "// TP N°1"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2d817d6e-2bfc-4d31-ac80-d1f2885d9c30",
   "metadata": {},
   "source": [
    " Création de l'environnement CartPole avec mode de rendu visuel\n",
    " Réinitialisation de l'environnement pour commencer un nouvel épisode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54001945-73d8-41fe-904b-7cec0d0c47f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.04222078, -0.01798953, -0.04799712,  0.04362695], dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d4812552-24ad-4e49-96d8-c341134057a6",
   "metadata": {},
   "source": [
    "Découverte et Exploration d'un Environnement Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbb41d96-b171-4cae-a3bd-d60994fe99af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espace d'actions : Discrete(2)\n",
      "Espace d'observations : Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n",
      "Action : 0, Observation : [-0.04258056 -0.21239154 -0.04712459  0.32078838], Reward : 1.0\n",
      "Action : 1, Observation : [-0.0468284  -0.01663126 -0.04070882  0.01362447], Reward : 1.0\n",
      "Action : 1, Observation : [-0.04716102  0.17905015 -0.04043633 -0.29161954], Reward : 1.0\n",
      "Action : 1, Observation : [-0.04358002  0.37472463 -0.04626872 -0.59677637], Reward : 1.0\n",
      "Action : 1, Observation : [-0.03608553  0.57046247 -0.05820425 -0.9036671 ], Reward : 1.0\n",
      "Action : 0, Observation : [-0.02467628  0.37617514 -0.07627759 -0.62983197], Reward : 1.0\n",
      "Action : 1, Observation : [-0.01715277  0.57227397 -0.08887423 -0.9455288 ], Reward : 1.0\n",
      "Action : 0, Observation : [-0.00570729  0.3784542  -0.10778481 -0.6820412 ], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.00186179  0.18498112 -0.12142563 -0.42514312], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.00556141  0.381595   -0.1299285  -0.7535039 ], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.01319331  0.18848094 -0.14499857 -0.504366  ], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.01696293  0.3853168  -0.15508589 -0.8390018 ], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.02466927  0.19261386 -0.17186593 -0.5988292 ], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.02852155  0.38967082 -0.18384251 -0.9403401 ], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.03631496  0.19743879 -0.20264931 -0.7105969 ], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.04026374  0.39470428 -0.21686125 -1.059612  ], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.00502857 -0.20952605  0.03567884  0.25475413], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.00083805 -0.4051388   0.04077392  0.5584738 ], Reward : 1.0\n",
      "Action : 1, Observation : [-0.00726473 -0.2106122   0.0519434   0.27891067], Reward : 1.0\n",
      "Action : 1, Observation : [-0.01147697 -0.01626824  0.05752161  0.00305242], Reward : 1.0\n",
      "Action : 0, Observation : [-0.01180234 -0.21216594  0.05758266  0.313315  ], Reward : 1.0\n",
      "Action : 0, Observation : [-0.01604566 -0.4080589   0.06384896  0.62358737], Reward : 1.0\n",
      "Action : 0, Observation : [-0.02420684 -0.6040115   0.07632071  0.93567616], Reward : 1.0\n",
      "Action : 0, Observation : [-0.03628707 -0.8000752   0.09503423  1.2513322 ], Reward : 1.0\n",
      "Action : 1, Observation : [-0.05228857 -0.6062907   0.12006088  0.98986506], Reward : 1.0\n",
      "Action : 0, Observation : [-0.06441438 -0.80279726  0.13985817  1.3177161 ], Reward : 1.0\n",
      "Action : 1, Observation : [-0.08047033 -0.60969347  0.1662125   1.0718731 ], Reward : 1.0\n",
      "Action : 1, Observation : [-0.0926642  -0.41711184  0.18764997  0.83562356], Reward : 1.0\n",
      "Action : 1, Observation : [-0.10100643 -0.22498049  0.20436244  0.60733163], Reward : 1.0\n",
      "Action : 1, Observation : [-0.10550604 -0.03321335  0.21650906  0.385332  ], Reward : 1.0\n",
      "Action : 1, Observation : [-0.04082123  0.22070384 -0.04275821 -0.32002953], Reward : 1.0\n",
      "Action : 0, Observation : [-0.03640715  0.02621608 -0.0491588  -0.04113163], Reward : 1.0\n",
      "Action : 0, Observation : [-0.03588283 -0.16816773 -0.04998143  0.23564498], Reward : 1.0\n",
      "Action : 0, Observation : [-0.03924618 -0.3625413  -0.04526853  0.5121528 ], Reward : 1.0\n",
      "Action : 1, Observation : [-0.04649701 -0.1668119  -0.03502548  0.20555508], Reward : 1.0\n",
      "Action : 1, Observation : [-0.04983325  0.02879296 -0.03091438 -0.09796764], Reward : 1.0\n",
      "Action : 0, Observation : [-0.04925739 -0.1658726  -0.03287373  0.1848038 ], Reward : 1.0\n",
      "Action : 0, Observation : [-0.05257484 -0.36050913 -0.02917765  0.46693775], Reward : 1.0\n",
      "Action : 0, Observation : [-0.05978502 -0.55520695 -0.0198389   0.7502831 ], Reward : 1.0\n",
      "Action : 1, Observation : [-0.07088916 -0.3598171  -0.00483323  0.45142376], Reward : 1.0\n",
      "Action : 0, Observation : [-0.0780855  -0.55487037  0.00419524  0.7425793 ], Reward : 1.0\n",
      "Action : 1, Observation : [-0.08918291 -0.35980657  0.01904683  0.45121956], Reward : 1.0\n",
      "Action : 1, Observation : [-0.09637904 -0.16495909  0.02807122  0.16460079], Reward : 1.0\n",
      "Action : 0, Observation : [-0.09967823 -0.3604714   0.03136323  0.46600556], Reward : 1.0\n",
      "Action : 1, Observation : [-0.10688765 -0.16580631  0.04068334  0.18337081], Reward : 1.0\n",
      "Action : 0, Observation : [-0.11020378 -0.36148605  0.04435076  0.48860517], Reward : 1.0\n",
      "Action : 1, Observation : [-0.1174335  -0.16701697  0.05412287  0.21022323], Reward : 1.0\n",
      "Action : 0, Observation : [-0.12077384 -0.36286935  0.05832733  0.51947594], Reward : 1.0\n",
      "Action : 0, Observation : [-0.12803122 -0.55876184  0.06871685  0.82995355], Reward : 1.0\n",
      "Action : 0, Observation : [-0.13920647 -0.75475246  0.08531592  1.1434325 ], Reward : 1.0\n",
      "Action : 1, Observation : [-0.15430151 -0.56084245  0.10818457  0.87867785], Reward : 1.0\n",
      "Action : 0, Observation : [-0.16551836 -0.75725496  0.12575813  1.2033185 ], Reward : 1.0\n",
      "Action : 0, Observation : [-0.18066347 -0.9537581   0.1498245   1.532621  ], Reward : 1.0\n",
      "Action : 0, Observation : [-0.19973862 -1.1503342   0.18047692  1.8680657 ], Reward : 1.0\n",
      "Action : 0, Observation : [-0.22274531 -1.346914    0.21783823  2.210919  ], Reward : 1.0\n",
      "Action : 1, Observation : [-0.02515767  0.17438382  0.01019435 -0.31655142], Reward : 1.0\n",
      "Action : 1, Observation : [-0.02166999  0.36935908  0.00386332 -0.60600203], Reward : 1.0\n",
      "Action : 1, Observation : [-0.01428281  0.5644268  -0.00825672 -0.8974656 ], Reward : 1.0\n",
      "Action : 0, Observation : [-0.00299428  0.36941776 -0.02620603 -0.60738945], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.00439408  0.5648961  -0.03835382 -0.90821   ], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.015692    0.37031373 -0.05651802 -0.62782437], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.02309828  0.56617707 -0.0690745  -0.9377577 ], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.03442182  0.37205106 -0.08782966 -0.6675548 ], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.04186284  0.1782532  -0.10118075 -0.4037673 ], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.0454279  -0.01529912 -0.1092561  -0.14462006], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.04512192  0.18120411 -0.1121485  -0.46967486], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.048746    0.3777168  -0.121542   -0.79549474], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.05630034  0.5742785  -0.13745189 -1.1238075 ], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.0677859   0.770908   -0.15992804 -1.4562538 ], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.08320407  0.5780691  -0.18905312 -1.2175053 ], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.09476545  0.38581932 -0.21340322 -0.98951995], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.03383638  0.24412234  0.01172646 -0.25097543], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.03871883  0.43907487  0.00670695 -0.53993666], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.04750033  0.6341019  -0.00409178 -0.83049875], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.06018237  0.43903613 -0.02070175 -0.53910553], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.06896309  0.24421123 -0.03148387 -0.25301668], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.07384732  0.43976825 -0.0365442  -0.5554616 ], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.08264268  0.6353837  -0.04765343 -0.8594305 ], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.09535035  0.83112127 -0.06484204 -1.1667082 ], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.11197278  1.0270243  -0.08817621 -1.4789954 ], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.13251327  1.2231051  -0.11775611 -1.7978642 ], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.15697536  1.0294814  -0.1537134  -1.5439805 ], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.177565   0.8365043 -0.184593  -1.3029417], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.19429508  1.0334245  -0.21065184 -1.6472664 ], Reward : 1.0\n",
      "Action : 1, Observation : [-0.01135398  0.21691865 -0.03891247 -0.33054876], Reward : 1.0\n",
      "Action : 0, Observation : [-0.00701561  0.0223716  -0.04552345 -0.05038645], Reward : 1.0\n",
      "Action : 0, Observation : [-0.00656818 -0.17206903 -0.04653117  0.22759305], Reward : 1.0\n",
      "Action : 1, Observation : [-0.01000956  0.02368594 -0.04197931 -0.07939681], Reward : 1.0\n",
      "Action : 0, Observation : [-0.00953584 -0.17080986 -0.04356725  0.19975157], Reward : 1.0\n",
      "Action : 1, Observation : [-0.01295204  0.02490725 -0.03957222 -0.10635025], Reward : 1.0\n",
      "Action : 0, Observation : [-0.01245389 -0.16962592 -0.04169922  0.17358989], Reward : 1.0\n",
      "Action : 1, Observation : [-0.01584641  0.02606725 -0.03822743 -0.13195074], Reward : 1.0\n",
      "Action : 0, Observation : [-0.01532506 -0.16848685 -0.04086644  0.14843109], Reward : 1.0\n",
      "Action : 1, Observation : [-0.0186948   0.02719575 -0.03789782 -0.15685914], Reward : 1.0\n",
      "Action : 0, Observation : [-0.01815088 -0.16736367 -0.041035    0.12363116], Reward : 1.0\n",
      "Action : 0, Observation : [-0.02149816 -0.36187446 -0.03856238  0.4030908 ], Reward : 1.0\n",
      "Action : 1, Observation : [-0.02873565 -0.1662274  -0.03050056  0.09850387], Reward : 1.0\n",
      "Action : 1, Observation : [-0.03206019  0.02931811 -0.02853049 -0.20364375], Reward : 1.0\n",
      "Action : 0, Observation : [-0.03147383 -0.16538446 -0.03260336  0.07990441], Reward : 1.0\n",
      "Action : 0, Observation : [-0.03478152 -0.3600242  -0.03100527  0.36212522], Reward : 1.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Espace d'actions : {env.action_space}\")\n",
    "print(f\"Espace d'observations : {env.observation_space}\")\n",
    "\n",
    "for _ in range(100):\n",
    "    action = env.action_space.sample()  \n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "    print(f\"Action : {action}, Observation : {observation}, Reward : {reward}\")\n",
    "    if done or truncated:\n",
    "        env.reset() \n",
    "\n",
    "env.close() "
   ]
  },
  {
   "cell_type": "raw",
   "id": "23a6614c-f817-44db-b91c-7f14b52dc6be",
   "metadata": {},
   "source": [
    "Manipulation des Observations et Récompenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93bf4a21-2cae-4d25-8543-b39c72c5c732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Étape 1:\n",
      "Action : 1\n",
      "Observation précédente : [ 0.04136518 -0.01255974  0.03797718  0.03231599]\n",
      "Nouvelle observation : [ 0.04111398  0.1819976   0.0386235  -0.248147  ]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 2:\n",
      "Action : 1\n",
      "Observation précédente : [ 0.04111398  0.1819976   0.0386235  -0.248147  ]\n",
      "Nouvelle observation : [ 0.04475393  0.37654728  0.03366056 -0.5284014 ]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 3:\n",
      "Action : 1\n",
      "Observation précédente : [ 0.04475393  0.37654728  0.03366056 -0.5284014 ]\n",
      "Nouvelle observation : [ 0.05228488  0.57117987  0.02309253 -0.8102906 ]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 4:\n",
      "Action : 0\n",
      "Observation précédente : [ 0.05228488  0.57117987  0.02309253 -0.8102906 ]\n",
      "Nouvelle observation : [ 0.06370848  0.3757493   0.00688672 -0.51043427]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 5:\n",
      "Action : 1\n",
      "Observation précédente : [ 0.06370848  0.3757493   0.00688672 -0.51043427]\n",
      "Nouvelle observation : [ 0.07122346  0.57077354 -0.00332197 -0.800939  ]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 6:\n",
      "Action : 1\n",
      "Observation précédente : [ 0.07122346  0.57077354 -0.00332197 -0.800939  ]\n",
      "Nouvelle observation : [ 0.08263893  0.7659409  -0.01934075 -1.094665  ]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 7:\n",
      "Action : 1\n",
      "Observation précédente : [ 0.08263893  0.7659409  -0.01934075 -1.094665  ]\n",
      "Nouvelle observation : [ 0.09795775  0.96131223 -0.04123405 -1.3933531 ]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 8:\n",
      "Action : 0\n",
      "Observation précédente : [ 0.09795775  0.96131223 -0.04123405 -1.3933531 ]\n",
      "Nouvelle observation : [ 0.117184    0.7667271  -0.06910111 -1.1138428 ]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 9:\n",
      "Action : 1\n",
      "Observation précédente : [ 0.117184    0.7667271  -0.06910111 -1.1138428 ]\n",
      "Nouvelle observation : [ 0.13251853  0.9626849  -0.09137797 -1.4273777 ]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 10:\n",
      "Action : 0\n",
      "Observation précédente : [ 0.13251853  0.9626849  -0.09137797 -1.4273777 ]\n",
      "Nouvelle observation : [ 0.15177223  0.76880294 -0.11992552 -1.1645958 ]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 11:\n",
      "Action : 1\n",
      "Observation précédente : [ 0.15177223  0.76880294 -0.11992552 -1.1645958 ]\n",
      "Nouvelle observation : [ 0.16714829  0.9652643  -0.14321744 -1.4923449 ]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 12:\n",
      "Action : 0\n",
      "Observation précédente : [ 0.16714829  0.9652643  -0.14321744 -1.4923449 ]\n",
      "Nouvelle observation : [ 0.18645358  0.77214605 -0.17306434 -1.2475954 ]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 13:\n",
      "Action : 0\n",
      "Observation précédente : [ 0.18645358  0.77214605 -0.17306434 -1.2475954 ]\n",
      "Nouvelle observation : [ 0.2018965   0.5796131  -0.19801624 -1.0137374 ]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 14:\n",
      "Action : 1\n",
      "Observation précédente : [ 0.2018965   0.5796131  -0.19801624 -1.0137374 ]\n",
      "Nouvelle observation : [ 0.21348876  0.7767458  -0.218291   -1.3614951 ]\n",
      "Récompense : 1.0\n",
      "Terminé ? : True\n",
      "Épisode terminé, réinitialisation...\n",
      "\n",
      "Étape 15:\n",
      "Action : 0\n",
      "Observation précédente : [-0.03751831  0.0366825  -0.00252557 -0.03384803]\n",
      "Nouvelle observation : [-0.03678466 -0.15840314 -0.00320253  0.25803697]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 16:\n",
      "Action : 1\n",
      "Observation précédente : [-0.03678466 -0.15840314 -0.00320253  0.25803697]\n",
      "Nouvelle observation : [-0.03995273  0.03676438  0.00195821 -0.03565435]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 17:\n",
      "Action : 1\n",
      "Observation précédente : [-0.03995273  0.03676438  0.00195821 -0.03565435]\n",
      "Nouvelle observation : [-0.03921743  0.2318582   0.00124512 -0.3277188 ]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 18:\n",
      "Action : 1\n",
      "Observation précédente : [-0.03921743  0.2318582   0.00124512 -0.3277188 ]\n",
      "Nouvelle observation : [-0.03458027  0.4269624  -0.00530926 -0.6200088 ]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 19:\n",
      "Action : 1\n",
      "Observation précédente : [-0.03458027  0.4269624  -0.00530926 -0.6200088 ]\n",
      "Nouvelle observation : [-0.02604102  0.6221581  -0.01770943 -0.91435915]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 20:\n",
      "Action : 1\n",
      "Observation précédente : [-0.02604102  0.6221581  -0.01770943 -0.91435915]\n",
      "Nouvelle observation : [-0.01359786  0.8175151  -0.03599662 -1.2125549 ]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 21:\n",
      "Action : 0\n",
      "Observation précédente : [-0.01359786  0.8175151  -0.03599662 -1.2125549 ]\n",
      "Nouvelle observation : [ 0.00275244  0.62287575 -0.06024772 -0.9313658 ]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 22:\n",
      "Action : 0\n",
      "Observation précédente : [ 0.00275244  0.62287575 -0.06024772 -0.9313658 ]\n",
      "Nouvelle observation : [ 0.01520995  0.4286163  -0.07887503 -0.6582074 ]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 23:\n",
      "Action : 1\n",
      "Observation précédente : [ 0.01520995  0.4286163  -0.07887503 -0.6582074 ]\n",
      "Nouvelle observation : [ 0.02378228  0.6247423  -0.09203918 -0.9746471 ]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 24:\n",
      "Action : 0\n",
      "Observation précédente : [ 0.02378228  0.6247423  -0.09203918 -0.9746471 ]\n",
      "Nouvelle observation : [ 0.03627713  0.4309675  -0.11153212 -0.71223646]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 25:\n",
      "Action : 1\n",
      "Observation précédente : [ 0.03627713  0.4309675  -0.11153212 -0.71223646]\n",
      "Nouvelle observation : [ 0.04489648  0.6274426  -0.12577686 -1.0378405 ]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 26:\n",
      "Action : 1\n",
      "Observation précédente : [ 0.04489648  0.6274426  -0.12577686 -1.0378405 ]\n",
      "Nouvelle observation : [ 0.05744533  0.8239913  -0.14653365 -1.3672155 ]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 27:\n",
      "Action : 1\n",
      "Observation précédente : [ 0.05744533  0.8239913  -0.14653365 -1.3672155 ]\n",
      "Nouvelle observation : [ 0.07392515  1.0206118  -0.17387797 -1.7019124 ]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 28:\n",
      "Action : 1\n",
      "Observation précédente : [ 0.07392515  1.0206118  -0.17387797 -1.7019124 ]\n",
      "Nouvelle observation : [ 0.09433739  1.2172579  -0.20791622 -2.0432968 ]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 29:\n",
      "Action : 0\n",
      "Observation précédente : [ 0.09433739  1.2172579  -0.20791622 -2.0432968 ]\n",
      "Nouvelle observation : [ 0.11868255  1.0247918  -0.24878216 -1.8215033 ]\n",
      "Récompense : 1.0\n",
      "Terminé ? : True\n",
      "Épisode terminé, réinitialisation...\n",
      "\n",
      "Étape 30:\n",
      "Action : 1\n",
      "Observation précédente : [-0.03699793 -0.02134237  0.02920648 -0.0391085 ]\n",
      "Nouvelle observation : [-0.03742477  0.17334886  0.02842431 -0.3224353 ]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 31:\n",
      "Action : 0\n",
      "Observation précédente : [-0.03742477  0.17334886  0.02842431 -0.3224353 ]\n",
      "Nouvelle observation : [-0.03395779 -0.02216608  0.02197561 -0.02092573]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 32:\n",
      "Action : 0\n",
      "Observation précédente : [-0.03395779 -0.02216608  0.02197561 -0.02092573]\n",
      "Nouvelle observation : [-0.03440112 -0.21759619  0.02155709  0.27860895]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 33:\n",
      "Action : 1\n",
      "Observation précédente : [-0.03440112 -0.21759619  0.02155709  0.27860895]\n",
      "Nouvelle observation : [-0.03875304 -0.02278829  0.02712927 -0.00719771]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 34:\n",
      "Action : 0\n",
      "Observation précédente : [-0.03875304 -0.02278829  0.02712927 -0.00719771]\n",
      "Nouvelle observation : [-0.03920881 -0.21828859  0.02698532  0.29391986]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 35:\n",
      "Action : 1\n",
      "Observation précédente : [-0.03920881 -0.21828859  0.02698532  0.29391986]\n",
      "Nouvelle observation : [-0.04357458 -0.02356156  0.03286371  0.00986838]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 36:\n",
      "Action : 0\n",
      "Observation précédente : [-0.04357458 -0.02356156  0.03286371  0.00986838]\n",
      "Nouvelle observation : [-0.04404581 -0.21913904  0.03306108  0.3127364 ]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 37:\n",
      "Action : 1\n",
      "Observation précédente : [-0.04404581 -0.21913904  0.03306108  0.3127364 ]\n",
      "Nouvelle observation : [-0.04842859 -0.02450329  0.03931581  0.03066049]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 38:\n",
      "Action : 1\n",
      "Observation précédente : [-0.04842859 -0.02450329  0.03931581  0.03066049]\n",
      "Nouvelle observation : [-0.04891866  0.17003344  0.03992902 -0.24936324]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 39:\n",
      "Action : 0\n",
      "Observation précédente : [-0.04891866  0.17003344  0.03992902 -0.24936324]\n",
      "Nouvelle observation : [-0.04551799 -0.0256353   0.03494176  0.05564196]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 40:\n",
      "Action : 0\n",
      "Observation précédente : [-0.04551799 -0.0256353   0.03494176  0.05564196]\n",
      "Nouvelle observation : [-0.04603069 -0.22124039  0.03605459  0.35914126]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 41:\n",
      "Action : 1\n",
      "Observation précédente : [-0.04603069 -0.22124039  0.03605459  0.35914126]\n",
      "Nouvelle observation : [-0.0504555  -0.02664903  0.04323742  0.07804169]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 42:\n",
      "Action : 1\n",
      "Observation précédente : [-0.0504555  -0.02664903  0.04323742  0.07804169]\n",
      "Nouvelle observation : [-0.05098848  0.16782728  0.04479825 -0.2006923 ]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 43:\n",
      "Action : 0\n",
      "Observation précédente : [-0.05098848  0.16782728  0.04479825 -0.2006923 ]\n",
      "Nouvelle observation : [-0.04763193 -0.02790581  0.04078441  0.10577904]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 44:\n",
      "Action : 0\n",
      "Observation précédente : [-0.04763193 -0.02790581  0.04078441  0.10577904]\n",
      "Nouvelle observation : [-0.04819005 -0.22358777  0.04289999  0.4110452 ]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 45:\n",
      "Action : 1\n",
      "Observation précédente : [-0.04819005 -0.22358777  0.04289999  0.4110452 ]\n",
      "Nouvelle observation : [-0.05266181 -0.02909942  0.05112089  0.13218981]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 46:\n",
      "Action : 1\n",
      "Observation précédente : [-0.05266181 -0.02909942  0.05112089  0.13218981]\n",
      "Nouvelle observation : [-0.0532438   0.1652544   0.05376469 -0.14393707]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 47:\n",
      "Action : 1\n",
      "Observation précédente : [-0.0532438   0.1652544   0.05376469 -0.14393707]\n",
      "Nouvelle observation : [-0.04993871  0.3595668   0.05088595 -0.4191853 ]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 48:\n",
      "Action : 1\n",
      "Observation précédente : [-0.04993871  0.3595668   0.05088595 -0.4191853 ]\n",
      "Nouvelle observation : [-0.04274737  0.55393213  0.04250224 -0.69540197]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 49:\n",
      "Action : 0\n",
      "Observation précédente : [-0.04274737  0.55393213  0.04250224 -0.69540197]\n",
      "Nouvelle observation : [-0.03166873  0.35824728  0.0285942  -0.3896478 ]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 50:\n",
      "Action : 1\n",
      "Observation précédente : [-0.03166873  0.35824728  0.0285942  -0.3896478 ]\n",
      "Nouvelle observation : [-0.02450378  0.552952    0.02080124 -0.6731799 ]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 51:\n",
      "Action : 0\n",
      "Observation précédente : [-0.02450378  0.552952    0.02080124 -0.6731799 ]\n",
      "Nouvelle observation : [-0.01344474  0.35754716  0.00733765 -0.374021  ]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 52:\n",
      "Action : 0\n",
      "Observation précédente : [-0.01344474  0.35754716  0.00733765 -0.374021  ]\n",
      "Nouvelle observation : [-6.2938002e-03  1.6232176e-01 -1.4277312e-04 -7.9033509e-02]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 53:\n",
      "Action : 0\n",
      "Observation précédente : [-6.2938002e-03  1.6232176e-01 -1.4277312e-04 -7.9033509e-02]\n",
      "Nouvelle observation : [-0.00304736 -0.03279814 -0.00172344  0.21360438]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 54:\n",
      "Action : 0\n",
      "Observation précédente : [-0.00304736 -0.03279814 -0.00172344  0.21360438]\n",
      "Nouvelle observation : [-0.00370333 -0.22789541  0.00254864  0.50574315]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 55:\n",
      "Action : 0\n",
      "Observation précédente : [-0.00370333 -0.22789541  0.00254864  0.50574315]\n",
      "Nouvelle observation : [-0.00826124 -0.42305318  0.01266351  0.79922813]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 56:\n",
      "Action : 1\n",
      "Observation précédente : [-0.00826124 -0.42305318  0.01266351  0.79922813]\n",
      "Nouvelle observation : [-0.0167223  -0.22810723  0.02864807  0.5105556 ]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 57:\n",
      "Action : 1\n",
      "Observation précédente : [-0.0167223  -0.22810723  0.02864807  0.5105556 ]\n",
      "Nouvelle observation : [-0.02128444 -0.03340031  0.03885918  0.22703648]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 58:\n",
      "Action : 1\n",
      "Observation précédente : [-0.02128444 -0.03340031  0.03885918  0.22703648]\n",
      "Nouvelle observation : [-0.02195245  0.16114537  0.04339991 -0.05314002]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 59:\n",
      "Action : 1\n",
      "Observation précédente : [-0.02195245  0.16114537  0.04339991 -0.05314002]\n",
      "Nouvelle observation : [-0.01872954  0.355619    0.04233711 -0.33182025]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 60:\n",
      "Action : 1\n",
      "Observation précédente : [-0.01872954  0.355619    0.04233711 -0.33182025]\n",
      "Nouvelle observation : [-0.01161716  0.55011356  0.03570071 -0.61085725]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 61:\n",
      "Action : 0\n",
      "Observation précédente : [-0.01161716  0.55011356  0.03570071 -0.61085725]\n",
      "Nouvelle observation : [-0.00061489  0.3545113   0.02348356 -0.307147  ]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 62:\n",
      "Action : 0\n",
      "Observation précédente : [-0.00061489  0.3545113   0.02348356 -0.307147  ]\n",
      "Nouvelle observation : [ 0.00647533  0.15906271  0.01734062 -0.00715145]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 63:\n",
      "Action : 1\n",
      "Observation précédente : [ 0.00647533  0.15906271  0.01734062 -0.00715145]\n",
      "Nouvelle observation : [ 0.00965659  0.35393175  0.01719759 -0.29431316]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 64:\n",
      "Action : 1\n",
      "Observation précédente : [ 0.00965659  0.35393175  0.01719759 -0.29431316]\n",
      "Nouvelle observation : [ 0.01673522  0.54880434  0.01131133 -0.581523  ]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 65:\n",
      "Action : 0\n",
      "Observation précédente : [ 0.01673522  0.54880434  0.01131133 -0.581523  ]\n",
      "Nouvelle observation : [ 2.7711309e-02  3.5352573e-01 -3.1912958e-04 -2.8529835e-01]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 66:\n",
      "Action : 0\n",
      "Observation précédente : [ 2.7711309e-02  3.5352573e-01 -3.1912958e-04 -2.8529835e-01]\n",
      "Nouvelle observation : [ 0.03478182  0.15840834 -0.0060251   0.0072839 ]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 67:\n",
      "Action : 1\n",
      "Observation précédente : [ 0.03478182  0.15840834 -0.0060251   0.0072839 ]\n",
      "Nouvelle observation : [ 0.03794999  0.35361618 -0.00587942 -0.2872939 ]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 68:\n",
      "Action : 1\n",
      "Observation précédente : [ 0.03794999  0.35361618 -0.00587942 -0.2872939 ]\n",
      "Nouvelle observation : [ 0.04502232  0.5488215  -0.0116253  -0.5818253 ]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 69:\n",
      "Action : 1\n",
      "Observation précédente : [ 0.04502232  0.5488215  -0.0116253  -0.5818253 ]\n",
      "Nouvelle observation : [ 0.05599875  0.7441044  -0.0232618  -0.87814766]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 70:\n",
      "Action : 0\n",
      "Observation précédente : [ 0.05599875  0.7441044  -0.0232618  -0.87814766]\n",
      "Nouvelle observation : [ 0.07088083  0.5493061  -0.04082476 -0.5928677 ]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 71:\n",
      "Action : 0\n",
      "Observation précédente : [ 0.07088083  0.5493061  -0.04082476 -0.5928677 ]\n",
      "Nouvelle observation : [ 0.08186696  0.3547787  -0.05268211 -0.31331885]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 72:\n",
      "Action : 0\n",
      "Observation précédente : [ 0.08186696  0.3547787  -0.05268211 -0.31331885]\n",
      "Nouvelle observation : [ 0.08896253  0.1604453  -0.05894849 -0.03770451]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 73:\n",
      "Action : 1\n",
      "Observation précédente : [ 0.08896253  0.1604453  -0.05894849 -0.03770451]\n",
      "Nouvelle observation : [ 0.09217144  0.35636085 -0.05970258 -0.34838822]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 74:\n",
      "Action : 0\n",
      "Observation précédente : [ 0.09217144  0.35636085 -0.05970258 -0.34838822]\n",
      "Nouvelle observation : [ 0.09929866  0.16213663 -0.06667034 -0.07511307]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 75:\n",
      "Action : 1\n",
      "Observation précédente : [ 0.09929866  0.16213663 -0.06667034 -0.07511307]\n",
      "Nouvelle observation : [ 0.10254139  0.3581479  -0.0681726  -0.3880633 ]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 76:\n",
      "Action : 0\n",
      "Observation précédente : [ 0.10254139  0.3581479  -0.0681726  -0.3880633 ]\n",
      "Nouvelle observation : [ 0.10970435  0.16405648 -0.07593387 -0.11762971]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 77:\n",
      "Action : 1\n",
      "Observation précédente : [ 0.10970435  0.16405648 -0.07593387 -0.11762971]\n",
      "Nouvelle observation : [ 0.11298547  0.36017966 -0.07828646 -0.43326986]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 78:\n",
      "Action : 1\n",
      "Observation précédente : [ 0.11298547  0.36017966 -0.07828646 -0.43326986]\n",
      "Nouvelle observation : [ 0.12018906  0.5563177  -0.08695186 -0.74956846]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 79:\n",
      "Action : 1\n",
      "Observation précédente : [ 0.12018906  0.5563177  -0.08695186 -0.74956846]\n",
      "Nouvelle observation : [ 0.13131543  0.7525245  -0.10194323 -1.0682985 ]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 80:\n",
      "Action : 0\n",
      "Observation précédente : [ 0.13131543  0.7525245  -0.10194323 -1.0682985 ]\n",
      "Nouvelle observation : [ 0.14636591  0.55888784 -0.1233092  -0.8092709 ]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 81:\n",
      "Action : 1\n",
      "Observation précédente : [ 0.14636591  0.55888784 -0.1233092  -0.8092709 ]\n",
      "Nouvelle observation : [ 0.15754366  0.7554642  -0.13949461 -1.1380576 ]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 82:\n",
      "Action : 0\n",
      "Observation précédente : [ 0.15754366  0.7554642  -0.13949461 -1.1380576 ]\n",
      "Nouvelle observation : [ 0.17265294  0.56241435 -0.16225576 -0.8921742 ]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 83:\n",
      "Action : 0\n",
      "Observation précédente : [ 0.17265294  0.56241435 -0.16225576 -0.8921742 ]\n",
      "Nouvelle observation : [ 0.18390124  0.36982104 -0.18009925 -0.6545729 ]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 84:\n",
      "Action : 0\n",
      "Observation précédente : [ 0.18390124  0.36982104 -0.18009925 -0.6545729 ]\n",
      "Nouvelle observation : [ 0.19129765  0.1776029  -0.19319071 -0.4235725 ]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 85:\n",
      "Action : 1\n",
      "Observation précédente : [ 0.19129765  0.1776029  -0.19319071 -0.4235725 ]\n",
      "Nouvelle observation : [ 0.19484971  0.37486145 -0.20166217 -0.77040124]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 86:\n",
      "Action : 1\n",
      "Observation précédente : [ 0.19484971  0.37486145 -0.20166217 -0.77040124]\n",
      "Nouvelle observation : [ 0.20234695  0.5721027  -0.21707019 -1.1191552 ]\n",
      "Récompense : 1.0\n",
      "Terminé ? : True\n",
      "Épisode terminé, réinitialisation...\n",
      "\n",
      "Étape 87:\n",
      "Action : 1\n",
      "Observation précédente : [-0.00895466  0.01288877  0.01213542 -0.01477953]\n",
      "Nouvelle observation : [-0.00869688  0.2078346   0.01183983 -0.303609  ]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 88:\n",
      "Action : 0\n",
      "Observation précédente : [-0.00869688  0.2078346   0.01183983 -0.303609  ]\n",
      "Nouvelle observation : [-0.00454019  0.01254593  0.00576765 -0.00721572]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 89:\n",
      "Action : 0\n",
      "Observation précédente : [-0.00454019  0.01254593  0.00576765 -0.00721572]\n",
      "Nouvelle observation : [-0.00428927 -0.18265826  0.00562333  0.28728136]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 90:\n",
      "Action : 1\n",
      "Observation précédente : [-0.00428927 -0.18265826  0.00562333  0.28728136]\n",
      "Nouvelle observation : [-0.00794244  0.01238305  0.01136896 -0.00362271]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 91:\n",
      "Action : 1\n",
      "Observation précédente : [-0.00794244  0.01238305  0.01136896 -0.00362271]\n",
      "Nouvelle observation : [-0.00769478  0.20734012  0.01129651 -0.292697  ]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 92:\n",
      "Action : 0\n",
      "Observation précédente : [-0.00769478  0.20734012  0.01129651 -0.292697  ]\n",
      "Nouvelle observation : [-0.00354798  0.01205894  0.00544256  0.00352715]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 93:\n",
      "Action : 0\n",
      "Observation précédente : [-0.00354798  0.01205894  0.00544256  0.00352715]\n",
      "Nouvelle observation : [-0.0033068  -0.18314064  0.00551311  0.29792228]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 94:\n",
      "Action : 0\n",
      "Observation précédente : [-0.0033068  -0.18314064  0.00551311  0.29792228]\n",
      "Nouvelle observation : [-0.00696961 -0.37834075  0.01147155  0.59233886]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 95:\n",
      "Action : 0\n",
      "Observation précédente : [-0.00696961 -0.37834075  0.01147155  0.59233886]\n",
      "Nouvelle observation : [-0.01453642 -0.5736214   0.02331833  0.8886131 ]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 96:\n",
      "Action : 1\n",
      "Observation précédente : [-0.01453642 -0.5736214   0.02331833  0.8886131 ]\n",
      "Nouvelle observation : [-0.02600885 -0.37882352  0.04109059  0.6033507 ]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 97:\n",
      "Action : 1\n",
      "Observation précédente : [-0.02600885 -0.37882352  0.04109059  0.6033507 ]\n",
      "Nouvelle observation : [-0.03358532 -0.18429963  0.05315761  0.32388842]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 98:\n",
      "Action : 0\n",
      "Observation précédente : [-0.03358532 -0.18429963  0.05315761  0.32388842]\n",
      "Nouvelle observation : [-0.03727132 -0.3801366   0.05963537  0.63284993]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 99:\n",
      "Action : 1\n",
      "Observation précédente : [-0.03727132 -0.3801366   0.05963537  0.63284993]\n",
      "Nouvelle observation : [-0.04487405 -0.1858951   0.07229237  0.359528  ]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n",
      "\n",
      "Étape 100:\n",
      "Action : 0\n",
      "Observation précédente : [-0.04487405 -0.1858951   0.07229237  0.359528  ]\n",
      "Nouvelle observation : [-0.04859195 -0.38196626  0.07948294  0.674102  ]\n",
      "Récompense : 1.0\n",
      "Terminé ? : False\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "observation, _ = env.reset()\n",
    "\n",
    "for step in range(100):\n",
    "    action = env.action_space.sample()\n",
    "    new_observation, reward, done, truncated, info = env.step(action)\n",
    "    \n",
    "    print(f\"\\nÉtape {step + 1}:\")\n",
    "    print(f\"Action : {action}\")\n",
    "    print(f\"Observation précédente : {observation}\")\n",
    "    print(f\"Nouvelle observation : {new_observation}\")\n",
    "    print(f\"Récompense : {reward}\")\n",
    "    print(f\"Terminé ? : {done or truncated}\")\n",
    "    \n",
    "    observation = new_observation\n",
    "    if done or truncated:\n",
    "        print(\"Épisode terminé, réinitialisation...\")\n",
    "        observation, _ = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e30b3c38-16c2-4db1-a4f4-c5fc5619a723",
   "metadata": {},
   "source": [
    "Contrôle Manuel de l'Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efc47b71-35cd-44aa-8838-c72b072aca65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contrôle manuel de l'agent CartPole\n",
      "Entrez 0 pour pousser à gauche, 1 pour pousser à droite\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Action (0 ou 1) :  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Étape 1:\n",
      "Observation : [-0.01520242 -0.2079387  -0.04483556  0.26231346]\n",
      "Récompense cumulée : 1.0\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Action (0 ou 1) :  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Étape 2:\n",
      "Observation : [-0.0193612  -0.40239292 -0.0395893   0.5405244 ]\n",
      "Récompense cumulée : 2.0\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Action (0 ou 1) :  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Étape 3:\n",
      "Observation : [-0.02740906 -0.20673752 -0.02877881  0.23563506]\n",
      "Récompense cumulée : 3.0\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Action (0 ou 1) :  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Étape 4:\n",
      "Observation : [-0.03154381 -0.40143672 -0.02406611  0.51910317]\n",
      "Récompense cumulée : 4.0\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Action (0 ou 1) :  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Étape 5:\n",
      "Observation : [-0.03957254 -0.59621173 -0.01368404  0.8041063 ]\n",
      "Récompense cumulée : 5.0\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Action (0 ou 1) :  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Étape 6:\n",
      "Observation : [-0.05149678 -0.7911434   0.00239808  1.0924535 ]\n",
      "Récompense cumulée : 6.0\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Action (0 ou 1) :  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Étape 7:\n",
      "Observation : [-0.06731965 -0.9862969   0.02424715  1.3858879 ]\n",
      "Récompense cumulée : 7.0\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Action (0 ou 1) :  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Étape 8:\n",
      "Observation : [-0.08704558 -1.1817126   0.05196491  1.6860533 ]\n",
      "Récompense cumulée : 8.0\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Action (0 ou 1) :  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Étape 9:\n",
      "Observation : [-0.11067984 -1.3773957   0.08568598  1.9944526 ]\n",
      "Récompense cumulée : 9.0\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Action (0 ou 1) :  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Étape 10:\n",
      "Observation : [-0.13822775 -1.5733035   0.12557504  2.312397  ]\n",
      "Récompense cumulée : 10.0\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Action (0 ou 1) :  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Étape 11:\n",
      "Observation : [-0.16969383 -1.7693291   0.17182297  2.640942  ]\n",
      "Récompense cumulée : 11.0\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Action (0 ou 1) :  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Étape 12:\n",
      "Observation : [-0.2050804  -1.9652845   0.22464181  2.9808147 ]\n",
      "Récompense cumulée : 12.0\n",
      "\n",
      "Épisode terminé après 12 pas!\n",
      "Récompense totale : 12.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "observation, _ = env.reset()\n",
    "total_reward = 0\n",
    "steps = 0\n",
    "\n",
    "print(\"Contrôle manuel de l'agent CartPole\")\n",
    "print(\"Entrez 0 pour pousser à gauche, 1 pour pousser à droite\")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        action = int(input(\"Action (0 ou 1) : \"))\n",
    "        if action not in [0, 1]:\n",
    "            print(\"Veuillez entrer 0 ou 1\")\n",
    "            continue\n",
    "            \n",
    "        observation, reward, done, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "        \n",
    "        print(f\"\\nÉtape {steps}:\")\n",
    "        print(f\"Observation : {observation}\")\n",
    "        print(f\"Récompense cumulée : {total_reward}\")\n",
    "        \n",
    "        if done or truncated:\n",
    "            print(f\"\\nÉpisode terminé après {steps} pas!\")\n",
    "            print(f\"Récompense totale : {total_reward}\")\n",
    "            break\n",
    "            \n",
    "    except ValueError:\n",
    "        print(\"Veuillez entrer un nombre (0 ou 1)\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41676f13-56ad-40f8-88e0-ee3e7bf4ff71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
